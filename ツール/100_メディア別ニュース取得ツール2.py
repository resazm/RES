import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import unicodedata
from concurrent.futures import ThreadPoolExecutor

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°", layout="wide")
st.title("ğŸ“¡ ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ„ãƒ¼ãƒ«")
st.write("ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ã—ã¾ã™ã€‚")

# ã‚«ãƒ†ã‚´ãƒªã¨URL
categories = {
    "æ—¥æœ¬æ ª": "https://finance.yahoo.co.jp/news/stocks",
    "å¸‚æ³ãƒ»æ¦‚æ³": "https://finance.yahoo.co.jp/news/market",
    "FX": "https://finance.yahoo.co.jp/news/fx",
    "å¤–å›½æ ª": "https://finance.yahoo.co.jp/news/world",
    "æ±ºç®—é€Ÿå ±": "https://finance.yahoo.co.jp/news/settlement"
}

# âœ… ã‚«ãƒ†ã‚´ãƒªé¸æŠãƒ»ãƒšãƒ¼ã‚¸æ•°
st.markdown("### âœ… ã‚«ãƒ†ã‚´ãƒªã¨ãƒšãƒ¼ã‚¸æ•°ã‚’é¸æŠ")
selected_categories = {}
cols = st.columns(5)
for idx, (name, url) in enumerate(categories.items()):
    with cols[idx]:
        if st.checkbox(name, value=True):
            pages = st.number_input(f"{name}ã®ãƒšãƒ¼ã‚¸æ•°", 1, 10, 2, 1, key=f"pg_{name}")
            selected_categories[name] = {"url": url, "pages": pages}

# ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š
st.markdown("### ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")
keyword_input = st.text_input("ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›ï¼ˆä¾‹ï¼šæ—¥çµŒå¹³å‡, å††å®‰, æ±ºç®—ï¼‰", "æ—¥çµŒå¹³å‡, å††å®‰, æ±ºç®—")
keywords = [k.strip() for k in keyword_input.split(",") if k.strip()]
filter_mode = st.selectbox("æ¤œç´¢å¯¾è±¡", ["ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿", "æœ¬æ–‡ã®ã¿", "ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡"])

# è¨˜äº‹è©³ç´°ã®å–å¾—
def fetch_article_detail(url):
    try:
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.content, "html.parser")
        time_elem = soup.find("p", class_=re.compile("time"))
        pub_time = time_elem.text.strip().replace("é…ä¿¡", "").strip() if time_elem else "ä¸æ˜"
        body_elem = soup.find(class_=re.compile("textArea"))
        body = unicodedata.normalize('NFKC', body_elem.text.strip()) if body_elem else ""
        return pub_time, body
    except Exception as e:
        return "ä¸æ˜", ""

# ã‚«ãƒ†ã‚´ãƒªã®è¨˜äº‹ä¸€è¦§å–å¾—
def fetch_articles(name, url, pages):
    articles = []
    for page in range(1, pages + 1):
        list_url = url if page == 1 else f"{url}?page={page}"
        try:
            res = requests.get(list_url, timeout=10)
            soup = BeautifulSoup(res.content, "html.parser")
            elems = soup.find_all(href=re.compile("finance.yahoo.co.jp/news/detail"))
            for elem in elems:
                title = elem.text.strip()
                link = elem.attrs["href"]
                pub_time, body = fetch_article_detail(link)
                articles.append({
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "URL": link,
                    "å…¬é–‹æ—¥æ™‚": pub_time,
                    "æœ¬æ–‡": body
                })
        except Exception as e:
            print(f"[{name}] ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼:", e)
    return name, articles

# ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°
def match_keywords(article, keywords, mode):
    title = article["ã‚¿ã‚¤ãƒˆãƒ«"]
    body = article["æœ¬æ–‡"]
    if not keywords:
        return True
    if mode == "ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿":
        return any(k in title for k in keywords)
    elif mode == "æœ¬æ–‡ã®ã¿":
        return any(k in body for k in keywords)
    else:
        return any(k in title or k in body for k in keywords)

# ğŸ”˜ å®Ÿè¡Œ
if st.button("ğŸ§² ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"):
    if not selected_categories:
        st.warning("ã‚«ãƒ†ã‚´ãƒªã‚’1ã¤ä»¥ä¸Šé¸ã‚“ã§ãã ã•ã„ã€‚")
    else:
        st.info("å–å¾—ä¸­...ï¼ˆå°‘ã—æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰")
        results = {}

        with ThreadPoolExecutor() as executor:
            futures = [
                executor.submit(fetch_articles, name, info["url"], info["pages"])
                for name, info in selected_categories.items()
            ]
            for future in futures:
                name, articles = future.result()
                results[name] = articles

        # ğŸ”½ è¡¨ç¤ºï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ã«5ã‚«ãƒ©ãƒ ï¼‰
        st.markdown("## ğŸ—‚ï¸ çµæœè¡¨ç¤º")
        col_outs = st.columns(5)
        for i, (cat, articles) in enumerate(results.items()):
            with col_outs[i]:
                st.subheader(cat)
                filtered = [a for a in articles if match_keywords(a, keywords, filter_mode)]
                if not filtered:
                    st.write("ğŸ“­ è©²å½“è¨˜äº‹ãªã—")
                else:
                    for art in filtered:
                        st.markdown(f"**[{art['ã‚¿ã‚¤ãƒˆãƒ«']}]({art['URL']})**")
                        st.caption(f"ğŸ•’ {art['å…¬é–‹æ—¥æ™‚']}")
                        with st.expander("ğŸ“– æœ¬æ–‡ã‚’è¡¨ç¤º"):
                            st.write(art["æœ¬æ–‡"])
                        st.markdown("---")

