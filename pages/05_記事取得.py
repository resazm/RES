import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import unicodedata

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°", layout="wide")

st.title("ğŸ“¡ ãƒ¡ãƒ‡ã‚£ã‚¢åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ„ãƒ¼ãƒ«")
st.write("ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã‚ºãƒ»ã‚¦ã‚§ãƒ–:[å¤§å¼•ã‘æ¦‚æ³]")
st.write("ã‚¦ã‚¨ãƒ«ã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼:[æ—¥çµŒå¹³å‡ã¯]")
st.write("æ ªæ¢:[æ—¥çµŒå¹³å‡])")
st.write("ãƒ­ã‚¤ã‚¿ãƒ¼:[åˆå‰ã®æ—¥çµŒå¹³å‡ã¯]")
st.write("ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")



# ãƒ¡ãƒ‡ã‚£ã‚¢è¨­å®š
media_sources = {
    "ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã‚ºãƒ»ã‚¦ã‚§ãƒ–": "https://finance.yahoo.co.jp/news/media/dzh",
    "ã‚¦ã‚¨ãƒ«ã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": "https://finance.yahoo.co.jp/news/media/mosf",
    "æ ªæ¢": "https://finance.yahoo.co.jp/news/media/stkms",
    "ãƒ­ã‚¤ã‚¿ãƒ¼": "https://finance.yahoo.co.jp/news/media/reut"
}

media_keywords = {
    "ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã‚ºãƒ»ã‚¦ã‚§ãƒ–": ["å¤§å¼•ã‘æ¦‚æ³"],
    "ãƒ­ã‚¤ã‚¿ãƒ¼": ["åˆå‰ã®æ—¥çµŒå¹³å‡ã¯"],
    "ã‚¦ã‚¨ãƒ«ã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": ["æ—¥çµŒå¹³å‡ã¯"],
    "æ ªæ¢": ["æ—¥çµŒå¹³å‡"]
}

def fetch_news():
    all_results = {}

    for media_name, base_url in media_sources.items():
        st.write(f"ğŸ“° {media_name} ã®è¨˜äº‹ã‚’å–å¾—ä¸­...")
        keywords = media_keywords.get(media_name, [])
        media_news = []

        for page in [1, 2, 3]:
            url = base_url if page == 1 else f"{base_url}?vip=off&page={page}"
            req = requests.get(url)
            soup = BeautifulSoup(req.content, "html.parser")
            elems = soup.find_all(href=re.compile("finance.yahoo.co.jp/news/detail"))

            for elem in elems:
                title = elem.text.strip()
                link = elem.attrs['href']
                pub_time = "ä¸æ˜"

                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
                if any(kw in title for kw in keywords):
                    try:
                        res_detail = requests.get(link)
                        soup_detail = BeautifulSoup(res_detail.content, "html.parser")

                        # å…¬é–‹æ™‚åˆ»ã‚’å–å¾—ï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ã® <p class="time_3Lu0">ï¼‰
                        time_elem = soup_detail.find("p", class_=re.compile("time"))
                        if time_elem:
                            pub_time = time_elem.text.strip().replace("é…ä¿¡", "").strip()

                        # æœ¬æ–‡
                        elems_detail = soup_detail.find(class_=re.compile("textArea"))
                        if elems_detail:
                            body = unicodedata.normalize('NFKC', elems_detail.text.strip())
                            media_news.append({
                                "ã‚¿ã‚¤ãƒˆãƒ«": title,
                                "URL": link,
                                "å…¬é–‹æ—¥æ™‚": pub_time,
                                "æœ¬æ–‡": body
                            })
                        time.sleep(1)
                    except Exception as e:
                        print(f"Error fetching detail: {e}")

        all_results[media_name] = media_news

    return all_results

# ãƒœã‚¿ãƒ³
if st.button("ğŸ§² ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"):
    results = fetch_news()

    # ã‚«ãƒ©ãƒ åˆ†å‰²ï¼ˆæœ€å¤§4ã¤ï¼‰
    media_list = list(media_sources.keys())
    cols = st.columns(4)

    for idx, media_name in enumerate(media_list):
        with cols[idx]:
            st.subheader(media_name)
            articles = results.get(media_name, [])
            if not articles:
                st.write("ğŸ“­ è©²å½“è¨˜äº‹ãªã—")
            else:
                for article in articles:
                    st.markdown(f"### [{article['ã‚¿ã‚¤ãƒˆãƒ«']}]({article['URL']})")
                    st.caption(f"ğŸ•’ å…¬é–‹æ—¥æ™‚: {article['å…¬é–‹æ—¥æ™‚']}")
                    st.write(article['æœ¬æ–‡'])
                    st.markdown("---")
