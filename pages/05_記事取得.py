import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import time
import unicodedata

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°", layout="wide", initial_sidebar_state="collapsed")

st.title("ğŸ“¡ ãƒ¡ãƒ‡ã‚£ã‚¢åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ„ãƒ¼ãƒ«")
st.write("ğŸ” å„ãƒ¡ãƒ‡ã‚£ã‚¢ã”ã¨ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒšãƒ¼ã‚¸æ•°ã‚’è‡ªç”±ã«è¨­å®šã—ã¦è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ãã¾ã™ã€‚")
st.write("ã€€æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹ã‚‚ã®ã‚’å–å¾—ã—ã¾ã™ã€‚2025/5/20 æ±ä½œæˆ")

# ãƒ¡ãƒ‡ã‚£ã‚¢ã¨URL
media_sources = {
    "ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã‚ºãƒ»ã‚¦ã‚§ãƒ–": "https://finance.yahoo.co.jp/news/media/dzh",
    "ã‚¦ã‚¨ãƒ«ã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": "https://finance.yahoo.co.jp/news/media/mosf",
    "æ ªæ¢": "https://finance.yahoo.co.jp/news/media/stkms",
    "ãƒ­ã‚¤ã‚¿ãƒ¼": "https://finance.yahoo.co.jp/news/media/reut"
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
default_keywords = {
    "ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã‚ºãƒ»ã‚¦ã‚§ãƒ–": ["å¤§å¼•ã‘æ¦‚æ³"],
    "ã‚¦ã‚¨ãƒ«ã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": ["æ—¥çµŒå¹³å‡ã¯"],
    "æ ªæ¢": ["æ—¥çµŒå¹³å‡"],
    "ãƒ­ã‚¤ã‚¿ãƒ¼": ["åˆå‰ã®æ—¥çµŒå¹³å‡ã¯"]
}

# ğŸ”§ ãƒ¡ãƒ‡ã‚£ã‚¢ã”ã¨ã®è¨­å®šï¼ˆãƒã‚§ãƒƒã‚¯ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ãƒšãƒ¼ã‚¸æ•°ï¼‰
selected_media = {}
custom_keywords = {}
custom_pages = {}

st.markdown("### âœ… ãƒ¡ãƒ‡ã‚£ã‚¢é¸æŠãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒšãƒ¼ã‚¸æ•°ã®è¨­å®š")

media_list = list(media_sources.keys())
media_cols = st.columns(4)

for idx, media in enumerate(media_list):
    with media_cols[idx]:
        st.markdown(f"#### {media}")
        use_media = st.checkbox("ã“ã®ãƒ¡ãƒ‡ã‚£ã‚¢ã‚’å–å¾—", value=True, key=f"use_{media}")
        if use_media:
            url = media_sources[media]
            selected_media[media] = url

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
            default_kw = ", ".join(default_keywords[media])
            user_input = st.text_input("ğŸ“ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰", value=default_kw, key=f"kw_{media}")
            custom_keywords[media] = [kw.strip() for kw in user_input.split(",") if kw.strip()]

            # ãƒšãƒ¼ã‚¸æ•°æŒ‡å®š
            pages = st.number_input("ğŸ“„ æ¤œç´¢ãƒšãƒ¼ã‚¸æ•°", min_value=1, max_value=20, value=3, step=1, key=f"pg_{media}")
            custom_pages[media] = pages

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°
def fetch_news(keywords_dict, pages_dict, active_sources):
    all_results = {}

    for media_name, base_url in active_sources.items():
        st.write(f"ğŸ“° {media_name} ã®è¨˜äº‹ã‚’å–å¾—ä¸­...")
        keywords = keywords_dict.get(media_name, [])
        max_page = pages_dict.get(media_name, 1)
        media_news = []

        for page in range(1, max_page + 1):
            url = base_url if page == 1 else f"{base_url}?vip=off&page={page}"
            try:
                req = requests.get(url, timeout=10)
                soup = BeautifulSoup(req.content, "html.parser")
                elems = soup.find_all(href=re.compile("finance.yahoo.co.jp/news/detail"))

                for elem in elems:
                    title = elem.text.strip()
                    link = elem.attrs['href']
                    pub_time = "ä¸æ˜"

                    if any(kw in title for kw in keywords):
                        try:
                            res_detail = requests.get(link, timeout=10)
                            soup_detail = BeautifulSoup(res_detail.content, "html.parser")

                            time_elem = soup_detail.find("p", class_=re.compile("time"))
                            if time_elem:
                                pub_time = time_elem.text.strip().replace("é…ä¿¡", "").strip()

                            body_elem = soup_detail.find(class_=re.compile("textArea"))
                            if body_elem:
                                body = unicodedata.normalize('NFKC', body_elem.text.strip())
                                media_news.append({
                                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                                    "URL": link,
                                    "å…¬é–‹æ—¥æ™‚": pub_time,
                                    "æœ¬æ–‡": body
                                })
                            time.sleep(1)
                        except Exception as e:
                            print(f"Error fetching detail page: {e}")
            except Exception as e:
                print(f"Error fetching list page: {e}")

        all_results[media_name] = media_news

    return all_results

# ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒœã‚¿ãƒ³
if st.button("ğŸ§² ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"):
    if not selected_media:
        st.warning("âš ï¸ å°‘ãªãã¨ã‚‚1ã¤ã®ãƒ¡ãƒ‡ã‚£ã‚¢ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    else:
        results = fetch_news(custom_keywords, custom_pages, selected_media)

        cols = st.columns(len(selected_media))
        for idx, media_name in enumerate(selected_media.keys()):
            with cols[idx]:
                st.subheader(media_name)
                articles = results.get(media_name, [])
                if not articles:
                    st.write("ğŸ“­ è©²å½“è¨˜äº‹ãªã—")
                else:
                    for article in articles:
                        st.markdown(f"### [{article['ã‚¿ã‚¤ãƒˆãƒ«']}]({article['URL']})")
                        st.caption(f"ğŸ•’ å…¬é–‹æ—¥æ™‚: {article['å…¬é–‹æ—¥æ™‚']}")
                        st.write(article['æœ¬æ–‡'])
                        st.markdown("---")
