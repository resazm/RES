import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import time
import unicodedata

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°", layout="wide", initial_sidebar_state="collapsed")

st.title("ğŸ“¡ ãƒ¡ãƒ‡ã‚£ã‚¢åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ„ãƒ¼ãƒ«")
st.write("ğŸ” ä¸‹è¨˜ã€æŒ‡å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¾ã™ï¼ˆç·¨é›†å¯ï¼‰")
st.write("ã€€æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹ã‚‚ã®ã‚’å–å¾—ã—ã¾ã™ã€‚2025/5/20 æ±ä½œæˆ")

# ãƒ¡ãƒ‡ã‚£ã‚¢ã¨URLè¨­å®š
media_sources = {
    "ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã‚ºãƒ»ã‚¦ã‚§ãƒ–": "https://finance.yahoo.co.jp/news/media/dzh",
    "ã‚¦ã‚¨ãƒ«ã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": "https://finance.yahoo.co.jp/news/media/mosf",
    "æ ªæ¢": "https://finance.yahoo.co.jp/news/media/stkms",
    "ãƒ­ã‚¤ã‚¿ãƒ¼": "https://finance.yahoo.co.jp/news/media/reut"
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š
default_keywords = {
    "ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã‚ºãƒ»ã‚¦ã‚§ãƒ–": ["å¤§å¼•ã‘æ¦‚æ³"],
    "ã‚¦ã‚¨ãƒ«ã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": ["æ—¥çµŒå¹³å‡ã¯"],
    "æ ªæ¢": ["æ—¥çµŒå¹³å‡"],
    "ãƒ­ã‚¤ã‚¿ãƒ¼": ["åˆå‰ã®æ—¥çµŒå¹³å‡ã¯"]
}

# ãƒšãƒ¼ã‚¸æ•°å…¥åŠ›
st.markdown("### ğŸ“„ æ¤œç´¢ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°")
num_pages = st.number_input("ãƒšãƒ¼ã‚¸æ•°ï¼ˆ1ï½20ï¼‰", min_value=1, max_value=20, value=3, step=1)

# ğŸ”§ ãƒ¡ãƒ‡ã‚£ã‚¢ã”ã¨ã®ãƒã‚§ãƒƒã‚¯ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
st.markdown("### âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®ãƒ¡ãƒ‡ã‚£ã‚¢ã¨è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸æŠ")

selected_media = {}
custom_keywords = {}

for media in media_sources.keys():
    col1, col2 = st.columns([1, 3])
    with col1:
        use_media = st.checkbox(f"{media} ã‚’å–å¾—", value=True)
    if use_media:
        with col2:
            default = ", ".join(default_keywords[media])
            user_input = st.text_input(f"ğŸ“ {media} ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value=default, key=media)
            selected_media[media] = media_sources[media]
            custom_keywords[media] = [kw.strip() for kw in user_input.split(",") if kw.strip()]

# ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–¢æ•°
def fetch_news(keywords_dict, max_page, active_sources):
    all_results = {}

    for media_name, base_url in active_sources.items():
        st.write(f"ğŸ“° {media_name} ã®è¨˜äº‹ã‚’å–å¾—ä¸­...")
        keywords = keywords_dict.get(media_name, [])
        media_news = []

        for page in range(1, max_page + 1):
            url = base_url if page == 1 else f"{base_url}?vip=off&page={page}"
            try:
                req = requests.get(url, timeout=10)
                soup = BeautifulSoup(req.content, "html.parser")
                elems = soup.find_all(href=re.compile("finance.yahoo.co.jp/news/detail"))

                for elem in elems:
                    title = elem.text.strip()
                    link = elem.attrs['href']
                    pub_time = "ä¸æ˜"

                    if any(kw in title for kw in keywords):
                        try:
                            res_detail = requests.get(link, timeout=10)
                            soup_detail = BeautifulSoup(res_detail.content, "html.parser")

                            # å…¬é–‹æ—¥æ™‚ã®å–å¾—
                            time_elem = soup_detail.find("p", class_=re.compile("time"))
                            if time_elem:
                                pub_time = time_elem.text.strip().replace("é…ä¿¡", "").strip()

                            # æœ¬æ–‡å–å¾—
                            body_elem = soup_detail.find(class_=re.compile("textArea"))
                            if body_elem:
                                body = unicodedata.normalize('NFKC', body_elem.text.strip())
                                media_news.append({
                                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                                    "URL": link,
                                    "å…¬é–‹æ—¥æ™‚": pub_time,
                                    "æœ¬æ–‡": body
                                })
                            time.sleep(1)
                        except Exception as e:
                            print(f"Error fetching detail page: {e}")
            except Exception as e:
                print(f"Error fetching list page: {e}")

        all_results[media_name] = media_news

    return all_results

# ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒœã‚¿ãƒ³
if st.button("ğŸ§² ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"):
    if not selected_media:
        st.warning("âš ï¸ å°‘ãªãã¨ã‚‚1ã¤ã®ãƒ¡ãƒ‡ã‚£ã‚¢ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    else:
        results = fetch_news(custom_keywords, num_pages, selected_media)

        # ã‚«ãƒ©ãƒ æ•°ã‚’å‹•çš„ã«åˆ†ã‘ã‚‹ï¼ˆé¸æŠãƒ¡ãƒ‡ã‚£ã‚¢æ•°ï¼‰
        cols = st.columns(len(selected_media))
        for idx, media_name in enumerate(selected_media.keys()):
            with cols[idx]:
                st.subheader(media_name)
                articles = results.get(media_name, [])
                if not articles:
                    st.write("ğŸ“­ è©²å½“è¨˜äº‹ãªã—")
                else:
                    for article in articles:
                        st.markdown(f"### [{article['ã‚¿ã‚¤ãƒˆãƒ«']}]({article['URL']})")
                        st.caption(f"ğŸ•’ å…¬é–‹æ—¥æ™‚: {article['å…¬é–‹æ—¥æ™‚']}")
                        st.write(article['æœ¬æ–‡'])
                        st.markdown("---")
