import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import time
import unicodedata
from concurrent.futures import ThreadPoolExecutor, as_completed
import datetime
import html

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°", layout="wide", initial_sidebar_state="collapsed")

st.title("ğŸ“¡ ãƒ¡ãƒ‡ã‚£ã‚¢åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ„ãƒ¼ãƒ«")
st.write("ğŸ” å„ãƒ¡ãƒ‡ã‚£ã‚¢ã”ã¨ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒšãƒ¼ã‚¸æ•°ã‚’è‡ªç”±ã«è¨­å®šã—ã¦è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ãã¾ã™ã€‚")
st.write("ã€€æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã€æœ¬æ–‡ã€ãã®ã©ã¡ã‚‰ã‹ã«å«ã¾ã‚Œã‚‹ã‚‚ã®ã‚’å–å¾—ã—ã¾ã™ã€‚2025/5/20 æ±ä½œæˆ")

# æ¤œç´¢å¯¾è±¡ã®é¸æŠ
search_mode = st.radio(
    "\U0001f50d ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œç´¢å¯¾è±¡ã‚’é¸æŠ",
    ["ã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹", "æœ¬æ–‡ã«å«ã¾ã‚Œã‚‹", "ã©ã¡ã‚‰ã‹ã«å«ã¾ã‚Œã‚‹"],
    index=2,  # â†ã€Œã©ã¡ã‚‰ã‹ã«å«ã¾ã‚Œã‚‹ã€ã‚’åˆæœŸé¸æŠ
    horizontal=True
)

# ãƒ¡ãƒ‡ã‚£ã‚¢è¨­å®š
media_sources = {
    "ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã‚ºãƒ»ã‚¦ã‚§ãƒ–": "https://finance.yahoo.co.jp/news/media/dzh",
    "ã‚¦ã‚¨ãƒ«ã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": "https://finance.yahoo.co.jp/news/media/mosf",
    "æ ªæ¢": "https://finance.yahoo.co.jp/news/media/stkms",
    "ãƒ­ã‚¤ã‚¿ãƒ¼": "https://finance.yahoo.co.jp/news/media/reut"
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
default_keywords = {
    "ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã‚ºãƒ»ã‚¦ã‚§ãƒ–": "å¤§å¼•ã‘æ¦‚æ³",
    "ãƒ­ã‚¤ã‚¿ãƒ¼": "åˆå‰ã®æ—¥çµŒå¹³å‡ã¯",
    "ã‚¦ã‚¨ãƒ«ã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": "æ—¥çµŒå¹³å‡ã¯",
    "æ ªæ¢": "å€¤ä¸ŠãŒã‚Š"
}

# ãƒ¡ãƒ‡ã‚£ã‚¢åˆ¥è¨­å®šãƒ•ã‚©ãƒ¼ãƒ 
st.markdown("### \U0001f4dd ãƒ¡ãƒ‡ã‚£ã‚¢é¸æŠãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒšãƒ¼ã‚¸æ•°ãƒ»å…¬é–‹æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ã®è¨­å®š")
media_cols = st.columns(4)
selected_media = {}

for idx, (media, url) in enumerate(media_sources.items()):
    with media_cols[idx]:
        enabled = st.checkbox(f" {media}", value=True)
        if enabled:
            keyword_input = st.text_input(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ({media})", value=default_keywords[media], key=f"kw_{media}")
            page_num = st.number_input(f"ãƒšãƒ¼ã‚¸æ•° ({media})", min_value=1, max_value=10, value=1, key=f"pg_{media}")
            time_default = "12:00" if media == "ãƒ­ã‚¤ã‚¿ãƒ¼" else ""
            time_filter = st.text_input(f"å…¬é–‹æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆHH:MMä»¥é™ã€ç©ºæ¬„ã§ç„¡åŠ¹ï¼‰ ({media})", value=time_default, key=f"time_{media}")
            selected_media[media] = {
                "url": url,
                "keywords": [kw.strip() for kw in keyword_input.split(",") if kw.strip()],
                "pages": page_num,
                "time_filter": time_filter.strip()
            }

def parse_time(pub_time_str):
    # å…¬é–‹æ—¥æ™‚ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ™‚åˆ»ã‚’æŠ½å‡ºï¼ˆä¾‹ï¼š"15:45"ã‚’datetime.timeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§è¿”ã™ï¼‰
    m = re.search(r'(\d{1,2}):(\d{2})', pub_time_str)
    if m:
        hour, minute = int(m.group(1)), int(m.group(2))
        return datetime.time(hour, minute)
    return None

def highlight_keywords(text, keywords):
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’HTMLã§ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆèƒŒæ™¯é»„è‰²ï¼‰ã—ã¦è¿”ã™
    def repl(m):
        return f"<mark>{html.escape(m.group(0))}</mark>"
    for kw in keywords:
        # æ­£è¦è¡¨ç¾ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—å…¨ã¦å¤§æ–‡å­—ãƒ»å°æ–‡å­—åŒºåˆ¥ãªãæ¤œç´¢
        pattern = re.compile(re.escape(kw), re.IGNORECASE)
        text = pattern.sub(repl, text)
    return text

def fetch_article_detail(link):
    try:
        res_detail = requests.get(link, timeout=10)
        soup_detail = BeautifulSoup(res_detail.content, "html.parser")

        time_elem = soup_detail.find("p", class_=re.compile("time"))
        pub_time = time_elem.text.strip().replace("é…ä¿¡", "").strip() if time_elem else "ä¸æ˜"

        body_elem = soup_detail.find(class_=re.compile("textArea"))
        body = unicodedata.normalize('NFKC', body_elem.text.strip()) if body_elem else ""

        return pub_time, body
    except Exception as e:
        print(f"Error fetching detail: {e}")
        return "ä¸æ˜", ""

def process_media(media_name, info, keywords, search_mode):
    base_url = info["url"]
    max_page = info["pages"]
    time_filter_str = info.get("time_filter", "")

    if time_filter_str:
        try:
            filter_hour, filter_minute = map(int, time_filter_str.split(":"))
            filter_time = datetime.time(filter_hour, filter_minute)
        except:
            filter_time = None
    else:
        filter_time = None

    media_news = []

    for page in range(1, max_page + 1):
        url = base_url if page == 1 else f"{base_url}?vip=off&page={page}"
        try:
            req = requests.get(url, timeout=10)
            soup = BeautifulSoup(req.content, "html.parser")
            elems = soup.find_all(href=re.compile("finance.yahoo.co.jp/news/detail"))

            # ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§è¨˜äº‹è©³ç´°ã‚’å–å¾—
            with ThreadPoolExecutor(max_workers=5) as executor:
                future_to_elem = {executor.submit(fetch_article_detail, elem.attrs['href']): elem for elem in elems}
                for future in as_completed(future_to_elem):
                    elem = future_to_elem[future]
                    title = elem.text.strip()
                    link = elem.attrs['href']

                    try:
                        pub_time, body = future.result()
                    except:
                        pub_time, body = "ä¸æ˜", ""

                    # å…¬é–‹æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿åˆ¤å®š
                    pub_time_obj = parse_time(pub_time)
                    if filter_time and pub_time_obj and pub_time_obj < filter_time:
                        continue  # æŒ‡å®šæ™‚é–“ã‚ˆã‚Šå‰ã®è¨˜äº‹ã¯ã‚¹ã‚­ãƒƒãƒ—

                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒåˆ¤å®š
                    keyword_matched = False
                    if search_mode == "ã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹":
                        keyword_matched = any(kw in title for kw in keywords)
                    elif search_mode == "æœ¬æ–‡ã«å«ã¾ã‚Œã‚‹":
                        keyword_matched = any(kw in body for kw in keywords)
                    elif search_mode == "ã©ã¡ã‚‰ã‹ã«å«ã¾ã‚Œã‚‹":
                        keyword_matched = any(kw in title or kw in body for kw in keywords)

                    if keyword_matched:
                        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚¤ãƒ©ã‚¤ãƒˆé©ç”¨ï¼ˆHTMLãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—ï¼‰
                        title_hl = highlight_keywords(html.escape(title), keywords)
                        body_hl = highlight_keywords(html.escape(body), keywords)

                        media_news.append({
                            "ã‚¿ã‚¤ãƒˆãƒ«": title_hl,
                            "URL": link,
                            "å…¬é–‹æ—¥æ™‚": pub_time,
                            "æœ¬æ–‡": body_hl
                        })

            time.sleep(1)
        except Exception as e:
            print(f"Error fetching list page: {e}")

    return media_news

# ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒœã‚¿ãƒ³
if st.button("\U0001fa99 ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"):
    results = {}
    # ä¸¦åˆ—ã§å„ãƒ¡ãƒ‡ã‚£ã‚¢å‡¦ç†
    with ThreadPoolExecutor(max_workers=len(selected_media)) as executor:
        futures = {
            executor.submit(process_media, media, info, info["keywords"], search_mode): media
            for media, info in selected_media.items()
        }
        for future in as_completed(futures):
            media_name = futures[future]
            try:
                results[media_name] = future.result()
            except Exception as e:
                results[media_name] = []
                st.error(f"{media_name} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    if not results:
        st.warning("âš ï¸ å–å¾—å¯¾è±¡ã®ãƒ¡ãƒ‡ã‚£ã‚¢ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    media_list = list(results.keys())
    cols = st.columns(len(media_list)) if media_list else []

    fixed_order = ["ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã‚ºãƒ»ã‚¦ã‚§ãƒ–", "ã‚¦ã‚¨ãƒ«ã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼", "æ ªæ¢", "ãƒ­ã‚¤ã‚¿ãƒ¼"]
    active_media = [m for m in fixed_order if m in results]
    cols = st.columns(len(active_media))


    for idx, media_name in enumerate(active_media):
        with cols[idx]:
            st.subheader(media_name)
            articles = results.get(media_name, [])
            if not articles:
                st.write("\U0001f4ed è©²å½“è¨˜äº‹ãªã—")
            else:
                for article in articles:
                    st.markdown(f"### <a href='{article['URL']}' target='_blank'>{article['ã‚¿ã‚¤ãƒˆãƒ«']}</a>", unsafe_allow_html=True)
                    st.caption(f"\U0001f552 å…¬é–‹æ—¥æ™‚: {article['å…¬é–‹æ—¥æ™‚']}")
                    st.markdown(article['æœ¬æ–‡'].replace("\n", "<br>"), unsafe_allow_html=True)
                    st.markdown("---")
